{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement graphlab\n",
      "ERROR: No matching distribution found for graphlab\n"
     ]
    }
   ],
   "source": [
    "! pip install graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphlab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0f297b11739c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphlab'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import graphlab as gl\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.graphlab\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories_sf = gl.load_sframe(\"hn_processed.sframe\")\n",
    "bows = stories_sf['bow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_model = gl.topic_model.create(bows, num_topics=100, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.graphlab.prepare(topic_model, bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fit more topics and the topics become more fine-grained. They become difficult to visualize in the intertopic map tough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_model150 = gl.topic_model.create(bows, num_topics=150, num_iterations=200)\n",
    "pyLDAvis.graphlab.prepare(topic_model150, bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pos_re = re.compile(r'/(NOUN|ADJ|VERB|ADV)')\n",
    "\n",
    "def extract_dists(model, sf=stories_sf):\n",
    "    data = pyLDAvis.graphlab._extract_data(model, sf['bow'])\n",
    "    vocab = data['vocab'] = [pos_re.sub('', t).replace('_', ' ') for t in data['vocab']]\n",
    "    vis_data = pyLDAvis.prepare(**data)\n",
    "    vis_topic_order = vis_data.topic_order\n",
    "    new_order = np.array(vis_topic_order) - 1\n",
    "    topic_ids = range(1, len(new_order) + 1)    \n",
    "    data['topic_term_dists'] = pd.DataFrame(data['topic_term_dists'].T, index=vocab)[new_order]\n",
    "    data['topic_term_dists'].columns = topic_ids\n",
    "    data['doc_topic_dists'] = pd.DataFrame(data['doc_topic_dists'], index=sf['title'])[new_order]\n",
    "    data['doc_topic_dists'].columns = topic_ids\n",
    "    if vis_data:\n",
    "        data['vis'] = vis_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_data = extract_dists(topic_model)\n",
    "\n",
    "def topics_for(doc_name, doc_dist=model_data['doc_topic_dists']):\n",
    "    return doc_dist.ix[doc_name].order(ascending=False)\n",
    "\n",
    "def _sort_cols(df, cols):\n",
    "    res = df[cols].apply(lambda probs: probs.order(ascending=False).index)\n",
    "    return res.reset_index(drop=True)\n",
    "\n",
    "def top_topic_terms(topic_ids, topic_term_dists=model_data['topic_term_dists']):\n",
    "    return _sort_cols(topic_term_dists, topic_ids)\n",
    "\n",
    "def top_docs(topic_ids, doc_topic_dists=model_data['doc_topic_dists']):\n",
    "    return _sort_cols(doc_topic_dists, topic_ids)\n",
    "\n",
    "def top_term_topics(term, topic_term_dists=model_data['topic_term_dists']):\n",
    "    df = topic_term_dists.T[term].order(ascending=False)\n",
    "    return df#.reset_index(drop=True)\n",
    "\n",
    "def all_top_terms(topic_term_dists=model_data['topic_term_dists']):\n",
    "    return top_topic_terms(topic_term_dists.columns)\n",
    "\n",
    "def topic_docs(topic_id, doc_topic_dists=model_data['doc_topic_dists']):\n",
    "    return doc_topic_dists[topic_id].order(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a focused model around 'code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code_topics = top_term_topics('code')[0:10]\n",
    "code_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code_topics = code_topics[code_topics > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(code_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_docs(code_topics.index).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code_topics = code_topics[code_topics > 0.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_docs(code_topics.index).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_ordred_by_code = model_data['doc_topic_dists'][code_topics.index].sum(axis=1).order(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_focused_model(ordered_docs, num_topics, num_iters=100, threshold=0.1):\n",
    "    subset = set(ordered_docs[ordered_docs > threshold].index)\n",
    "    print('Keeping %.2f%% of the corpus...' % (100 * (len(subset) / len(ordered_docs))))\n",
    "    # I should have kept the doc index around, oh well..\n",
    "    stories_subset = stories_sf[stories_sf['title'].apply(lambda t: t in subset)]\n",
    "    bows = stories_subset['bow']    \n",
    "    print('Fitting model...')\n",
    "    tm = gl.topic_model.create(bows, num_topics, num_iterations=num_iters)\n",
    "    print('Creating vis data...')\n",
    "    data = extract_dists(tm, stories_subset)\n",
    "    data['model'] = tm\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code_model = fit_focused_model(docs_ordred_by_code, 40, num_iters=500, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code_model['vis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of looking at a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_topics = topics_for('Game written by 14 year old passes Angry Birds as the top free iphone app').head(4)\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without LDAvis you would then look at the top words for those docs.. something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_topic_terms(top_topics.index)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the all of the topics you are reduced to looking at a wall of words or tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_top_terms().head(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a17a13407a242fa5e52f5f0d84ee969a8e600591a999235f1645da9e037e8a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "name": "Interpreting a model old school.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}